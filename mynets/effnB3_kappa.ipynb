{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.kaggle.com/nobletp/panda-keras-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test: python3 effnB3kappa_da_wb.py --cnnpar effnB3_test --mfolder effnB2kappa_deepaug\n",
    "#train: python3 effnB2kappa_da_wb.py --cnnpar effnB3_da --mfolder effnB3kappa_da_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.2.0\n",
      "no gpus\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, argparse, sys\n",
    "sys.path.append(\"..\")\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import json\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import efficientnet.tfkeras as efn\n",
    "print('tensorflow version:', tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('no gpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import albumentations as albu\n",
    "# from imgaug import augmenters as iaa\n",
    "# import imgaug as ia\n",
    "# ia.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imgaug import augmenters as iaa\n",
    "# sometimes = lambda aug: iaa.Sometimes(0.5, aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panda_bvv_config\n",
    "from panda_bvv_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FOR TRAIN:\n",
    "# ap = argparse.ArgumentParser()\n",
    "# ap.add_argument('--cnnpar', type=str, help=\"parameters name\", dest = 'cnn_parameters', default = 'effnB2')\n",
    "# ap.add_argument('--mfolder', help=\"folder to save model files\", dest = 'mfolder', default = 'effnB2_model',\\\n",
    "#                 type=str)\n",
    "#                 #(\"--cnn\", type=str, help=\"training cnn name\", dest = 'train_cnn')\n",
    "# args = vars(ap.parse_args())\n",
    "# cnnet = args[\"cnn_parameters\"]\n",
    "# model_save_folder = args[\"mfolder\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR TEST:\n",
    "cnnet = 'effnB3_test'\n",
    "model_save_folder = 'effnB3kappa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_save_folder_path = os.path.join(note_path, model_save_folder)\n",
    "input_parameters = panda_bvv_config.train_dict.get(cnnet)\n",
    "\n",
    "image_sizey = input_parameters.get('image_sizey')\n",
    "image_sizex = input_parameters.get('image_sizex')\n",
    "num_epochs = input_parameters.get('num_epochs')\n",
    "num_reduceOnPlateu = input_parameters.get('num_reduceOnPlateu')\n",
    "learn_rate = input_parameters.get('learn_rate')\n",
    "stop_patience = input_parameters.get('stop_patience')\n",
    "inp_label_smooth = input_parameters.get('inp_label_smooth')\n",
    "our_id_label_map = input_parameters.get('id_label_map')\n",
    "class_weights_ = input_parameters.get('class_weights')\n",
    "output_bias = tf.keras.initializers.Constant(input_parameters.get('output_bias'))\n",
    "BS = input_parameters.get('BS')\n",
    "s_per_epoch = input_parameters.get('s_per_epoch')\n",
    "val_steps = input_parameters.get('val_steps')\n",
    "model_name = input_parameters.get('model_name')\n",
    "checkpoint_name = input_parameters.get('checkpoint_name')\n",
    "weights_file = input_parameters.get('weights_file')\n",
    "file_for_struct = input_parameters.get('file_for_struct')\n",
    "file_for_weights = input_parameters.get('file_for_weights')\n",
    "history_file = input_parameters.get('history_file')\n",
    "save_plot_file = input_parameters.get('save_plot_file')\n",
    "num_logits = input_parameters.get('num_logits')\n",
    "from_folder_train = os.path.join(base_path, input_parameters.get('from_folder_train'))\n",
    "from_folder_val = os.path.join(base_path, input_parameters.get('from_folder_val'))\n",
    "if input_parameters.get('bestmodel_weights'): bestmodel_weights = input_parameters.get('bestmodel_weights')\n",
    "input_shape_ =(image_sizey, image_sizex , 3)\n",
    "TrDataGen = input_parameters.get('trdatagen') \n",
    "ValDataGen = input_parameters.get('valdatagen') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_addons.metrics import  CohenKappa\n",
    "kappa_keras = CohenKappa(num_classes=num_logits,weightage='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO:] validation samples from val folder gs2_16x320\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(glob(from_folder_train +'/*.png'), \n",
    "                              test_size= val_size_proportion, \n",
    "                              random_state=random_state_split)\n",
    "\n",
    "if from_folder_val: \n",
    "    _, val = train_test_split(glob(from_folder_val +'/*.png'), \n",
    "                              test_size= val_size_proportion, \n",
    "                              random_state=random_state_split)\n",
    "    print(f\"[INFO:] validation samples from val folder {input_parameters.get('from_folder_val')}\")\n",
    "else:\n",
    "    print(\"[INFO:] validation samples from train folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_model_save_folder_path = os.path.join(note_path, model_save_folder)\n",
    "if not os.path.exists(full_model_save_folder_path):\n",
    "    print(\"[INFO] 'creating {}' directory\".format(model_save_folder))\n",
    "    os.makedirs(full_model_save_folder_path)\n",
    "model_name = os.path.join(full_model_save_folder_path, model_name)\n",
    "checkpoint_name = os.path.join(full_model_save_folder_path, checkpoint_name)\n",
    "\n",
    "weights_file = os.path.join(note_path, weights_file) #!:not the same path\n",
    "\n",
    "file_for_struct = os.path.join(full_model_save_folder_path, file_for_struct)\n",
    "file_for_weights = os.path.join(full_model_save_folder_path, file_for_weights)\n",
    "history_file = os.path.join(full_model_save_folder_path, history_file)\n",
    "save_plot_file_main = os.path.join(full_model_save_folder_path, 'acc_' + save_plot_file)\n",
    "save_plot_file_kappa = os.path.join(full_model_save_folder_path, 'kappa_' + save_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input parameters:\n",
      "\n",
      "image_sizey: 320\n",
      "image_sizex: 320\n",
      "num_epochs: 2\n",
      "num_earlyStop: 2\n",
      "num_reduceOnPlateu: 8\n",
      "learn_rate: 0.0005\n",
      "stop_patience: 14\n",
      "inp_label_smooth: 0.01\n",
      "BS: 10\n",
      "s_per_epoch: 20\n",
      "val_steps: 8\n",
      "class_weights: {0: 3.67082, 1: 3.982, 2: 7.90469, 3: 8.5475, 4: 8.4996, 5: 8.6732}\n",
      "output_bias: [2.448 2.367 1.681 1.603 1.608 1.588]\n",
      "model_name: model_panda.h5\n",
      "checkpoint_name: model_effnB3_panda_check\n",
      "weights_file: efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "bestmodel_weights: best12_weights.h5\n",
      "level0_file: None\n",
      "file_for_struct: model_effnB3_panda_struct.json\n",
      "file_for_weights: model_effnB3_panda_weights.json\n",
      "history_file: history_effnB3.json\n",
      "save_plot_file: plot_edu_effnb3.png\n",
      "from_folder_train: gs2_16x320\n",
      "from_folder_val: gs2_16x320\n",
      "num_logits: 6\n",
      "trdatagen: <function MultiClass.<locals>.wrapper at 0x7f656ea80d90>\n",
      "valdatagen: <function MultiClass.<locals>.wrapper at 0x7f656ea80d90>\n"
     ]
    }
   ],
   "source": [
    "print('CNN input parameters:\\n') \n",
    "for k, v in input_parameters.items():\n",
    "    if k != 'id_label_map':\n",
    "        print('{}: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(full_model_save_folder_path, 'parameters_' + model_name.split('.')[0]), 'w') as ff:\n",
    "    for k, v in input_parameters.items():\n",
    "        if k != 'id_label_map':\n",
    "            ff.write(f'{k}:{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = TrDataGen(\n",
    "#         train_cnn,\n",
    "#         image_sizey, image_sizex,\n",
    "#         batch_size_= BS,\n",
    "#         shuffle_=True)\n",
    "# val_datagen = ValDataGen(\n",
    "#         valid_cnn,\n",
    "#         image_sizey, \n",
    "#         image_sizex,\n",
    "#         batch_size_ = BS,\n",
    "#         shuffle_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = TrDataGen(list_files = train,\n",
    "                    id_label_map = our_id_label_map, \n",
    "                    batch_size = BS,\n",
    "                    depth = num_logits,\n",
    "                    augment=True,\n",
    "                    shuf = False)\n",
    "val_datagen = ValDataGen(list_files = val, \n",
    "                    id_label_map = our_id_label_map, \n",
    "                    batch_size = BS, \n",
    "                    depth = num_logits,\n",
    "                    augment=False,\n",
    "                    shuf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skimage.io.imshow(tt[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "#         EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "#         monitor='val_loss',\n",
    "#         mode = 'min',\n",
    "#         min_delta=1e-2,\n",
    "#         patience=stop_patience,\n",
    "#         verbose=1,\n",
    "#         restore_best_weights = True\n",
    "#         ),\n",
    "        \n",
    "        ModelCheckpoint(\n",
    "        filepath= checkpoint_name +\".{epoch:02d}.h5\",\n",
    "        monitor='val_loss',\n",
    "        mode = 'auto',\n",
    "        save_weights_only = False,\n",
    "        save_freq = 'epoch',\n",
    "        save_best_only=False\n",
    "        ),\n",
    "    \n",
    "        ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=.1,\n",
    "        patience = num_reduceOnPlateu,\n",
    "        verbose=1,\n",
    "        min_lr=1e-7\n",
    "        )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = efn.EfficientNetB3(\n",
    "    input_shape=input_shape_,\n",
    "    weights= weights_file,\n",
    "    include_top=False, \n",
    "    pooling='avg'\n",
    ")\n",
    "bottleneck = Model(inputs=bottleneck.inputs, outputs=bottleneck.layers[-2].output)\n",
    "model = Sequential()\n",
    "model.add(bottleneck)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(num_logits, activation='softmax', bias_initializer=output_bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input_parameters.get('bestmodel_weights'): model.load_weights(bestmodel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model (Model)                (None, 10, 10, 1536)      10783528  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 11,581,742\n",
      "Trainable params: 11,490,350\n",
      "Non-trainable params: 91,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt =  tf.keras.optimizers.Adam(lr=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss  = tf.keras.losses.CategoricalCrossentropy(label_smoothing = inp_label_smooth),\n",
    "    #loss='categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['categorical_accuracy', kappa_keras]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-07af640b7baa>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/2\n",
      "20/20 [==============================] - 139s 7s/step - loss: 12.6282 - categorical_accuracy: 0.2400 - cohen_kappa: 0.2172 - val_loss: 1.7081 - val_categorical_accuracy: 0.2625 - val_cohen_kappa: 0.2899 - lr: 5.0000e-04\n",
      "Epoch 2/2\n",
      "20/20 [==============================] - 168s 8s/step - loss: 10.3958 - categorical_accuracy: 0.2900 - cohen_kappa: 0.4163 - val_loss: 1.6732 - val_categorical_accuracy: 0.3250 - val_cohen_kappa: 0.2011 - lr: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen,\n",
    "    steps_per_epoch=s_per_epoch,\n",
    "    validation_data=val_datagen,\n",
    "    validation_steps=val_steps,\n",
    "    class_weight=class_weights_,\n",
    "    callbacks=callbacks_list,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_save = {}\n",
    "for k, v in history.history.items():\n",
    "    dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\n",
    "with open(history_file, 'w') as file:\n",
    "    json.dump(dict_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)\n",
    "model.save_weights(file_for_weights, save_format=\"h5\")\n",
    "json_config = model.to_json()\n",
    "with open(file_for_struct, 'w') as f:\n",
    "    json.dump(json_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(np.arange(0, num_epochs), history.history['loss'], label='loss')\n",
    "plt.plot(np.arange(0, num_epochs), history.history['val_loss'], label='val_loss')\n",
    "plt.plot(np.arange(0, num_epochs),history.history['categorical_accuracy'], label='cat. accuracy')\n",
    "plt.plot(np.arange(0, num_epochs),history.history['val_categorical_accuracy'], label='val_accuracy')\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(save_plot_file_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kappa\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(np.arange(0, num_epochs), history.history['cohen_kappa'], label='kappa')\n",
    "plt.plot(np.arange(0, num_epochs), history.history['val_cohen_kappa'], label='val_kappa')\n",
    "\n",
    "plt.title(\"Training Cohen Kappa on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Cohen Kappa\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(save_plot_file_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "module_name = 'effnB3_kappa'\n",
    "\n",
    "os.system('jupyter nbconvert --to python ' + module_name + '.ipynb')\n",
    "with open(module_name + '.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open(module_name + '.py', 'w') as f:\n",
    "    for line in lines:\n",
    "        if 'nbconvert --to python' in line:\n",
    "            break\n",
    "        else:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
