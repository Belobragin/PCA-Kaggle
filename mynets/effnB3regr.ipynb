{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For test:\n",
    "DEBUG = True\n",
    "\n",
    "#For train:\n",
    "#DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.kaggle.com/nobletp/panda-keras-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test: python3 effnB3regr.py --cnnpar effnB3regr_test --mfolder effnB3test\n",
    "#train: python3 effnB3regr.py --cnnpar effnB3regr --mfolder effnB3regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.2.0\n",
      "no gpus\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os, argparse, sys\n",
    "sys.path.append(\"..\")\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import pandas as pd \n",
    "import json\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import efficientnet.tfkeras as efn\n",
    "print('tensorflow version:', tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('no gpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_KERAS'] = '1'\n",
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panda_bvv_config\n",
    "from panda_bvv_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    #FOR TEST:\n",
    "    cnnet = 'effnB3regr_test'\n",
    "    model_save_folder = 'effnB3regr'\n",
    "else:\n",
    "    #FOR TRAIN:\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument('--cnnpar', type=str, help=\"parameters name\", dest = 'cnn_parameters', default = 'effnB2')\n",
    "    ap.add_argument('--mfolder', help=\"folder to save model files\", dest = 'mfolder', default = 'effnB2_model',\\\n",
    "                    type=str)\n",
    "                    #(\"--cnn\", type=str, help=\"training cnn name\", dest = 'train_cnn')\n",
    "    args = vars(ap.parse_args())\n",
    "    cnnet = args[\"cnn_parameters\"]\n",
    "    model_save_folder = args[\"mfolder\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_save_folder_path = os.path.join(note_path, model_save_folder)\n",
    "input_parameters = panda_bvv_config.train_dict.get(cnnet)\n",
    "\n",
    "image_sizey = input_parameters.get('image_sizey')\n",
    "image_sizex = input_parameters.get('image_sizex')\n",
    "num_epochs = input_parameters.get('num_epochs')\n",
    "num_reduceOnPlateu = input_parameters.get('num_reduceOnPlateu')\n",
    "learn_rate = input_parameters.get('learn_rate')\n",
    "stop_patience = input_parameters.get('stop_patience')\n",
    "inp_label_smooth = input_parameters.get('inp_label_smooth')\n",
    "our_id_label_map = input_parameters.get('id_label_map')\n",
    "class_weights_ = input_parameters.get('class_weights')\n",
    "output_bias = tf.keras.initializers.Constant(input_parameters.get('output_bias'))\n",
    "BS = input_parameters.get('BS')\n",
    "s_per_epoch = input_parameters.get('s_per_epoch')\n",
    "val_steps = input_parameters.get('val_steps')\n",
    "model_name = input_parameters.get('model_name')\n",
    "checkpoint_name = input_parameters.get('checkpoint_name')\n",
    "weights_file = input_parameters.get('weights_file')\n",
    "file_for_struct = input_parameters.get('file_for_struct')\n",
    "file_for_weights = input_parameters.get('file_for_weights')\n",
    "history_file = input_parameters.get('history_file')\n",
    "save_plot_file = input_parameters.get('save_plot_file')\n",
    "num_logits = input_parameters.get('num_logits')\n",
    "from_folder = os.path.join(base_path, input_parameters.get('from_folder'))\n",
    "#if input_parameters.get('bestmodel_weights'): \n",
    "bestmodel_weights = input_parameters.get('bestmodel_weights')\n",
    "#if input_parameters.get('level0_file'): \n",
    "level0_file = input_parameters.get('level0_file')\n",
    "input_shape_ =(image_sizey, image_sizex , 3)\n",
    "TrDataGen = input_parameters.get('trdatagen') \n",
    "ValDataGen = input_parameters.get('valdatagen') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_and_labels(model, generator):\n",
    "    \"\"\"\n",
    "    Get predictions and labels from the generator\n",
    "    \n",
    "    :param model: A Keras model object\n",
    "    :param generator: A Keras ImageDataGenerator object\n",
    "    \n",
    "    :return: A tuple with two Numpy Arrays. One containing the predictions\n",
    "    and one containing the labels\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for _ in range(val_steps):\n",
    "        x, y = next(generator)\n",
    "        preds.append(model.predict(x))\n",
    "        labels.append(y)\n",
    "    # Flatten list of numpy arrays\n",
    "    return np.concatenate(preds).ravel(), np.concatenate(labels).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CohCap(Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback for saving the best model\n",
    "    according to the Quadratic Weighted Kappa (QWK) metric\n",
    "    \"\"\"\n",
    "    metrics_name = os.path.join(full_model_save_folder_path,'kohen_kappa_best.h5')\n",
    "    kappa_history_file = os.path.join(full_model_save_folder_path,'kappa_history')\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Initialize list of QWK scores on validation data\n",
    "        \"\"\"\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Gets QWK score on the validation data\n",
    "        \n",
    "        :param epoch: The current epoch number\n",
    "        \"\"\"\n",
    "        # Get predictions and convert to integers\n",
    "        y_pred, labels = get_preds_and_labels(model, val_datagen)\n",
    "        y_pred = np.rint(y_pred).astype(np.uint8).clip(0, 4)\n",
    "        # We can use sklearns implementation of QWK straight out of the box\n",
    "        # as long as we specify weights as 'quadratic'\n",
    "        _val_kappa = cohen_kappa_score(labels, y_pred, weights='quadratic')\n",
    "        self.val_kappas.append(_val_kappa)\n",
    "        with open(self.kappa_history_file, 'a') as kh:\n",
    "                kh.write(f'{_val_kappa}\\n')\n",
    "        print(f\"val_kappa: {round(_val_kappa, 4)}\")        \n",
    "        if _val_kappa == max(self.val_kappas):\n",
    "            print(\"Validation Kappa has improved. Saving model.\")\n",
    "            self.model.save(self.metrics_name)\n",
    "            \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_metrics = CohCap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(glob(from_folder +'/*.png'), \n",
    "                              test_size= val_size_proportion, \n",
    "                              random_state=random_state_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN input parameters:\n",
      "\n",
      "image_sizey: 320\n",
      "image_sizex: 320\n",
      "num_epochs: 2\n",
      "num_earlyStop: 2\n",
      "num_reduceOnPlateu: 8\n",
      "learn_rate: 0.0005\n",
      "stop_patience: 14\n",
      "inp_label_smooth: 0.01\n",
      "BS: 10\n",
      "s_per_epoch: 20\n",
      "val_steps: 8\n",
      "class_weights: {0: 1.0, 1: 1.0847711927981996, 2: 2.1533879374534624, 4: 2.3154523618895118, 3: 2.3285024154589373, 5: 2.3627450980392157}\n",
      "output_bias: [2.448 2.367 1.681 1.603 1.608 1.588]\n",
      "model_name: model_panda.h5\n",
      "checkpoint_name: model_effnB3_panda_check\n",
      "weights_file: efficientnet-b3_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "bestmodel_weights: None\n",
      "level0_file: effnB3_check20_best_level0_weights.npy\n",
      "file_for_struct: model_effnB3_panda_struct.json\n",
      "file_for_weights: model_effnB3_panda_weights.json\n",
      "history_file: history_effnB3.json\n",
      "save_plot_file: plot_edu_effnb3.png\n",
      "from_folder: testdata320/testf\n",
      "num_logits: 6\n",
      "trdatagen: <function LinRegr.<locals>.wrapper at 0x7f615a5dfd90>\n",
      "valdatagen: <function LinRegr.<locals>.wrapper at 0x7f615a5dfd90>\n"
     ]
    }
   ],
   "source": [
    "print('CNN input parameters:\\n') \n",
    "for k, v in input_parameters.items():\n",
    "    if k != 'id_label_map':\n",
    "        print('{}: {}'.format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_model_save_folder_path = os.path.join(note_path, model_save_folder)\n",
    "if not os.path.exists(full_model_save_folder_path):\n",
    "    print(\"[INFO] 'creating {}' directory\".format(model_save_folder))\n",
    "    os.makedirs(full_model_save_folder_path)\n",
    "model_name = os.path.join(full_model_save_folder_path, model_name)\n",
    "checkpoint_name = os.path.join(full_model_save_folder_path, checkpoint_name)\n",
    "\n",
    "weights_file = os.path.join(note_path, weights_file) #!:not the same path\n",
    "\n",
    "file_for_struct = os.path.join(full_model_save_folder_path, file_for_struct)\n",
    "file_for_weights = os.path.join(full_model_save_folder_path, file_for_weights)\n",
    "history_file = os.path.join(full_model_save_folder_path, history_file)\n",
    "save_plot_file_main = os.path.join(full_model_save_folder_path, 'acc_' + save_plot_file)\n",
    "save_plot_file_kappa = os.path.join(full_model_save_folder_path, 'kappa_' + save_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = TrDataGen(\n",
    "#         train_cnn,\n",
    "#         image_sizey, image_sizex,\n",
    "#         batch_size_= BS,\n",
    "#         shuffle_=True)\n",
    "# val_datagen = ValDataGen(\n",
    "#         valid_cnn,\n",
    "#         image_sizey, \n",
    "#         image_sizex,\n",
    "#         batch_size_ = BS,\n",
    "#         shuffle_=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = TrDataGen(list_files = train,\n",
    "                    id_label_map = our_id_label_map, \n",
    "                    batch_size = BS,\n",
    "                    depth = num_logits,\n",
    "                    augment=True,\n",
    "                    shuf = False)\n",
    "val_datagen = ValDataGen(list_files = val, \n",
    "                    id_label_map = our_id_label_map, \n",
    "                    batch_size = BS, \n",
    "                    depth = num_logits,\n",
    "                    augment=False,\n",
    "                    shuf = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skimage.io.imshow(tt[0][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "#         EarlyStopping(\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "#         monitor='val_loss',\n",
    "#         mode = 'min',\n",
    "#         min_delta=1e-2,\n",
    "#         patience=stop_patience,\n",
    "#         verbose=1,\n",
    "#         restore_best_weights = True\n",
    "#         ),\n",
    "        \n",
    "        ModelCheckpoint(\n",
    "        filepath= checkpoint_name +\".{epoch:02d}.h5\",\n",
    "        monitor='val_loss',\n",
    "        mode = 'auto',\n",
    "        save_weights_only = False,\n",
    "        save_freq = 'epoch',\n",
    "        save_best_only=False\n",
    "        ),\n",
    "    \n",
    "        ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=.5,\n",
    "        patience = num_reduceOnPlateu,\n",
    "        verbose=1,\n",
    "        min_lr=1e-7,\n",
    "        epsilon=0.0001,\n",
    "        ),\n",
    "    kappa_metrics \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = efn.EfficientNetB3(\n",
    "    input_shape=input_shape_,\n",
    "    weights= None,\n",
    "    include_top=False, \n",
    "    pooling='avg'\n",
    ")\n",
    "bottleneck = Model(inputs=bottleneck.inputs, outputs=bottleneck.layers[-2].output)\n",
    "model = Sequential()\n",
    "model.add(bottleneck)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(num_logits, activation= 'elu', bias_initializer=output_bias))\n",
    "model.add(Dense(1, activation=\"linear\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if bestmodel_weights: model.load_weights(bestmodel_weights)\n",
    "if level0_file: model.layers[0].set_weights(\\\n",
    "                                np.load(level0_file, allow_pickle=True)\n",
    "                                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "model (Model)                (None, 10, 10, 1536)      10783528  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 1536)              6144      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               786944    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 11,581,749\n",
      "Trainable params: 11,490,357\n",
      "Non-trainable params: 91,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RAdam(total_steps=5000, warmup_proportion=0.1, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='mse',\n",
    "    #loss='categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['mse', 'acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_datagen,\n",
    "    steps_per_epoch=s_per_epoch,\n",
    "    validation_data=val_datagen,\n",
    "    validation_steps=val_steps,\n",
    "    class_weight=class_weights_,\n",
    "    callbacks=callbacks_list,\n",
    "    epochs=num_epochs,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_save = {}\n",
    "for k, v in history.history.items():\n",
    "    dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\n",
    "with open(history_file, 'w') as file:\n",
    "    json.dump(dict_to_save, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_name)\n",
    "model.save_weights(file_for_weights, save_format=\"h5\")\n",
    "json_config = model.to_json()\n",
    "with open(file_for_struct, 'w') as f:\n",
    "    json.dump(json_config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(np.arange(0, num_epochs), history.history['mse'], label='mse')\n",
    "plt.plot(np.arange(0, num_epochs), history.history['val_mse'], label='val_mse')\n",
    "plt.plot(np.arange(0, num_epochs),history.history['acc'], label='accuracy')\n",
    "plt.plot(np.arange(0, num_epochs),history.history['val_acc'], label='val_accuracy')\n",
    "plt.plot(np.arange(0, num_epochs),history.history['loss'], label='loss')\n",
    "plt.plot(np.arange(0, num_epochs),history.history['val_loss'], label='val_loss')\n",
    "plt.title(\"Training Loss, MSE and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"MSE/Accuracy/Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(save_plot_file_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #kappa\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "\n",
    "# plt.plot(np.arange(0, num_epochs), history.history['val_kappa'], label='validation kappa')\n",
    "# #plt.plot(np.arange(0, num_epochs), history.history['val_cohen_kappa'], label='val_kappa')\n",
    "\n",
    "# plt.title(\"Training Cohen Kappa on Dataset\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Cohen Kappa\")\n",
    "# plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(save_plot_file_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "module_name = 'effnB3regr'\n",
    "\n",
    "os.system('jupyter nbconvert --to python ' + module_name + '.ipynb')\n",
    "with open(module_name + '.py', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "with open(module_name + '.py', 'w') as f:\n",
    "    for line in lines:\n",
    "        if 'nbconvert --to python' in line:\n",
    "            break\n",
    "        else:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
